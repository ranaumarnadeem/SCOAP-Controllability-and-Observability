#!/usr/bin/env python3
"""
OpenTestability Tool Environment
A professional circuit testability analysis tool similar to Cadence Genus.

Usage:
    opentest> parse -i input.v -o output.json
    opentest> scoap -i input.json
    opentest> reconv -i input.json
    opentest> help
    opentest> exit
"""

import sys
import os
import argparse
import readline
from pathlib import Path
from typing import Dict, List, Optional

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from opentestability.core.dag_builder import create_dag_from_netlist
from opentestability.core.reconvergence import analyze_reconvergence
from opentestability.core.advanced_reconvergence import analyze_with_advanced_reconvergence
from opentestability.core.simple_reconvergence import analyze_with_simple_reconvergence
from opentestability.core.scoap import run as calculate_scoap_metrics
from opentestability.visualization.graph_renderer import visualize_gate_graph
from opentestability.utils.file_utils import get_project_paths, ensure_directory


class OpenTestEnvironment:
    """OpenTestability Tool Environment - Professional circuit analysis tool."""
    
    def __init__(self):
        self.paths = get_project_paths()
        self.verbose = False
        self.current_directory = Path.cwd()
        
    def print_banner(self):
        """Print tool banner."""
        print("=" * 60)
        print("OpenTestability Tool Environment v1.0")
        print("Professional Circuit Testability Analysis")
        print("=" * 60)
        print("Type 'help' for available commands")
        print("Type 'exit' to quit")
        print("-" * 60)
    
    def get_default_output(self, input_file: str, command: str) -> str:
        """Get default output filename based on input and command."""
        base = Path(input_file).stem
        if command == "parse":
            return f"{base}_parsed.json"
        elif command == "scoap":
            return f"{base}_scoap.json"
        elif command == "reconv":
            return f"{base}_reconv.json"
        elif command == "simple":
            return f"{base}_simple_reconv.json"
        elif command == "advanced":
            return f"{base}_advanced_reconv.json"
        elif command == "visualize":
            return f"{base}_graph.png"
        else:
            return f"{base}_out.json"
    
    def get_default_directory(self, command: str) -> Path:
        """Get default output directory for command."""
        if command == "parse":
            return self.paths['parsed']
        elif command == "scoap":
            return self.paths['results']
        elif command == "reconv" or command == "simple" or command == "advanced":
            return self.paths['reconvergence_output']
        elif command == "visualize":
            return self.paths['graphs']
        else:
            return self.paths['results']
    
    def parse_command(self, cmd_line: str) -> Dict:
        """Parse command line arguments."""
        try:
            # Split command and arguments
            parts = cmd_line.strip().split()
            if not parts:
                return {"command": "empty"}
            
            command = parts[0]
            args = parts[1:]
            
            # Parse arguments
            parser = argparse.ArgumentParser(description=f"OpenTestability {command} command")
            
            if command in ["parse", "scoap", "reconv", "simple", "advanced", "visualize"]:
                parser.add_argument("-i", "--input", required=True, help="Input file")
                parser.add_argument("-o", "--output", help="Output file (optional)")
                parser.add_argument("-d", "--directory", help="Output directory (optional)")
                parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
                
            elif command == "compare":
                parser.add_argument("-i", "--input", required=True, help="Input DAG file")
                parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
                
            elif command == "help":
                parser.add_argument("topic", nargs="?", help="Help topic")
                
            elif command in ["status", "clear", "pwd", "ls", "cd"]:
                parser.add_argument("args", nargs="*", help="Command arguments")
                
            parsed_args = parser.parse_args(args)
            return {"command": command, "args": parsed_args}
            
        except SystemExit:
            return {"command": "error", "error": "Invalid arguments"}
        except Exception as e:
            return {"command": "error", "error": str(e)}
    
    def execute_parse(self, args) -> bool:
        """Execute parse command."""
        input_file = args.input
        output_file = args.output or self.get_default_output(input_file, "parse")
        output_dir = args.directory or self.get_default_directory("parse")
        
        if self.verbose:
            print(f"Parsing: {input_file}")
            print(f"Output: {output_dir / output_file}")
        
        try:
            # Ensure output directory exists
            ensure_directory(output_dir)
            
            # Parse the file
            from opentestability.parsers.verilog_parser import parse_verilog_file
            input_path = self.paths['input'] / input_file
            output_path = output_dir / output_file
            
            parse_verilog_file(input_path, output_path)
            print(f"[✓] Parsed {input_file} -> {output_path}")
            return True
            
        except Exception as e:
            print(f"[✗] Error parsing {input_file}: {e}")
            return False
    
    def execute_scoap(self, args) -> bool:
        """Execute SCOAP analysis command."""
        input_file = args.input
        output_file = args.output or self.get_default_output(input_file, "scoap")
        output_dir = args.directory or self.get_default_directory("scoap")
        
        if self.verbose:
            print(f"Running SCOAP analysis on: {input_file}")
            print(f"Output: {output_dir / output_file}")
        
        try:
            ensure_directory(output_dir)
            # SCOAP expects input from parsed directory and outputs to results
            output_path = calculate_scoap_metrics(input_file, output_file, True)  # json_flag=True
            print(f"[✓] SCOAP analysis completed: {output_path}")
            return True
            
        except Exception as e:
            print(f"[✗] Error in SCOAP analysis: {e}")
            return False
    
    def execute_reconv(self, args) -> bool:
        """Execute basic reconvergence analysis."""
        input_file = args.input
        
        if self.verbose:
            print(f"Running reconvergence analysis on: {input_file}")
        
        try:
            output_path = analyze_reconvergence(input_file)
            print(f"[✓] Reconvergence analysis completed: {output_path}")
            return True
            
        except Exception as e:
            print(f"[✗] Error in reconvergence analysis: {e}")
            return False
    
    def execute_simple(self, args) -> bool:
        """Execute simple reconvergence analysis."""
        input_file = args.input
        
        if self.verbose:
            print(f"Running simple reconvergence analysis on: {input_file}")
        
        try:
            output_path = analyze_with_simple_reconvergence(input_file)
            print(f"[✓] Simple reconvergence analysis completed: {output_path}")
            return True
            
        except Exception as e:
            print(f"[✗] Error in simple reconvergence analysis: {e}")
            return False
    
    def execute_advanced(self, args) -> bool:
        """Execute advanced reconvergence analysis."""
        input_file = args.input
        
        if self.verbose:
            print(f"Running advanced reconvergence analysis on: {input_file}")
        
        try:
            output_path = analyze_with_advanced_reconvergence(input_file)
            print(f"[✓] Advanced reconvergence analysis completed: {output_path}")
            return True
            
        except Exception as e:
            print(f"[✗] Error in advanced reconvergence analysis: {e}")
            return False
    
    def execute_compare(self, args) -> bool:
        """Execute algorithm comparison."""
        input_file = args.input
        
        if self.verbose:
            print(f"Running algorithm comparison on: {input_file}")
        
        try:
            # Run all algorithms
            print("Running baseline reconvergence...")
            baseline_output = analyze_reconvergence(input_file)
            
            print("Running simple reconvergence...")
            simple_output = analyze_with_simple_reconvergence(input_file)
            
            print("Running advanced reconvergence...")
            advanced_output = analyze_with_advanced_reconvergence(input_file)
            
            # Create comparison report
            comparison_report = self.create_comparison_report(
                baseline_output, simple_output, advanced_output, input_file
            )
            
            print(f"[✓] Comparison completed: {comparison_report}")
            return True
            
        except Exception as e:
            print(f"[✗] Error in comparison: {e}")
            return False
    
    def create_comparison_report(self, baseline_output, simple_output, advanced_output, input_file):
        """Create comparison report."""
        import json
        import time
        
        # Load results
        with open(baseline_output, 'r') as f:
            baseline_results = json.load(f)
        with open(simple_output, 'r') as f:
            simple_results = json.load(f)
        with open(advanced_output, 'r') as f:
            advanced_results = json.load(f)
        
        # Create comparison
        comparison = {
            'input_file': input_file,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'algorithms': {
                'baseline': {
                    'name': 'Baseline Reconvergence',
                    'reconvergences': len(baseline_results),
                    'file': str(baseline_output)
                },
                'simple': {
                    'name': 'Simple Reconvergence',
                    'reconvergences': simple_results.get('total_reconvergences', 0),
                    'file': str(simple_output)
                },
                'advanced': {
                    'name': 'Advanced Reconvergence',
                    'reconvergences': len(advanced_results.get('reconvergences', [])),
                    'file': str(advanced_output)
                }
            }
        }
        
        # Save comparison
        base = Path(input_file).stem
        comparison_file = self.paths['reconvergence_output'] / f"{base}_comparison.json"
        
        with open(comparison_file, 'w') as f:
            json.dump(comparison, f, indent=2)
        
        # Print summary
        print("\nComparison Summary:")
        print(f"  Baseline: {comparison['algorithms']['baseline']['reconvergences']} reconvergences")
        print(f"  Simple:   {comparison['algorithms']['simple']['reconvergences']} reconvergences")
        print(f"  Advanced: {comparison['algorithms']['advanced']['reconvergences']} reconvergences")
        
        return str(comparison_file)
    
    def execute_visualize(self, args) -> bool:
        """Execute visualization command."""
        input_file = args.input
        output_file = args.output or self.get_default_output(input_file, "visualize")
        output_dir = args.directory or self.get_default_directory("visualize")
        
        if self.verbose:
            print(f"Generating visualization for: {input_file}")
            print(f"Output: {output_dir / output_file}")
        
        try:
            ensure_directory(output_dir)
            output_path = visualize_gate_graph(input_file, output_file, output_dir)
            print(f"[✓] Visualization generated: {output_path}")
            return True
            
        except Exception as e:
            print(f"[✗] Error generating visualization: {e}")
            return False
    
    def show_help(self, topic: str = None):
        """Show help information."""
        if topic is None:
            print("\nOpenTestability Commands:")
            print("  parse     - Parse Verilog netlist")
            print("  scoap     - Calculate SCOAP testability metrics")
            print("  reconv    - Basic reconvergence detection")
            print("  simple    - Simple reconvergence detection")
            print("  advanced  - Advanced reconvergence detection")
            print("  compare   - Compare all algorithms")
            print("  visualize - Generate circuit visualization")
            print("  status    - Show project status")
            print("  help      - Show this help")
            print("  exit      - Exit tool environment")
            print("\nUse 'help <command>' for detailed command help")
            
        elif topic == "parse":
            print("\nparse - Parse Verilog netlist")
            print("Usage: parse -i <input.v> [-o <output.json>] [-d <directory>] [-v]")
            print("  -i, --input     Input Verilog file (required)")
            print("  -o, --output    Output file (default: <input>_parsed.json)")
            print("  -d, --directory Output directory (default: parsednetlist/)")
            print("  -v, --verbose   Verbose output")
            
        elif topic == "scoap":
            print("\nscoap - Calculate SCOAP testability metrics")
            print("Usage: scoap -i <input.json> [-o <output.json>] [-d <directory>] [-v]")
            print("  -i, --input     Input DAG file (required)")
            print("  -o, --output    Output file (default: <input>_scoap.json)")
            print("  -d, --directory Output directory (default: scoap/)")
            print("  -v, --verbose   Verbose output")
            
        elif topic in ["reconv", "simple", "advanced"]:
            print(f"\n{topic} - Reconvergence detection")
            print(f"Usage: {topic} -i <input.json> [-o <output.json>] [-d <directory>] [-v]")
            print("  -i, --input     Input DAG file (required)")
            print(f"  -o, --output    Output file (default: <input>_{topic}_reconv.json)")
            print("  -d, --directory Output directory (default: reconvergence/)")
            print("  -v, --verbose   Verbose output")
            
        elif topic == "compare":
            print("\ncompare - Compare all algorithms")
            print("Usage: compare -i <input.json> [-v]")
            print("  -i, --input     Input DAG file (required)")
            print("  -v, --verbose   Verbose output")
            print("\nThis command runs all three reconvergence algorithms and creates a comparison report.")
            
        elif topic == "visualize":
            print("\nvisualize - Generate circuit visualization")
            print("Usage: visualize -i <input.json> [-o <output.png>] [-d <directory>] [-v]")
            print("  -i, --input     Input DAG file (required)")
            print("  -o, --output    Output file (default: <input>_graph.png)")
            print("  -d, --directory Output directory (default: graphs/)")
            print("  -v, --verbose   Verbose output")
            print("\nThis command generates a visual representation of the circuit graph.")
            
        elif topic == "status":
            print("\nstatus - Show project status")
            print("Usage: status")
            print("\nThis command displays the current project status including:")
            print("  - Current working directory")
            print("  - Input and output directory paths")
            print("  - File counts in each directory")
            
        else:
            print(f"Unknown help topic: {topic}")
    
    def show_status(self):
        """Show project status."""
        print("\nOpenTestability Project Status:")
        print(f"  Current directory: {self.current_directory}")
        print(f"  Input directory: {self.paths['input']}")
        print(f"  Output directories:")
        print(f"    Parsed: {self.paths['parsed']}")
        print(f"    Results: {self.paths['results']}")
        print(f"    Reconvergence: {self.paths['reconvergence_output']}")
        print(f"    Graphs: {self.paths['graphs']}")
        print(f"    DAG: {self.paths['dag_output']}")
        
        # Count files in each directory
        for name, path in self.paths.items():
            if path.exists() and path.is_dir() and name in ['input', 'parsed', 'results', 'reconvergence_output', 'graphs', 'dag_output']:
                count = len(list(path.glob("*")))
                print(f"    {name}: {count} files")
    
    def run(self):
        """Run the tool environment."""
        self.print_banner()
        
        while True:
            try:
                # Get command
                cmd_line = input("opentest> ").strip()
                
                if not cmd_line:
                    continue
                
                # Parse command
                parsed = self.parse_command(cmd_line)
                
                if parsed["command"] == "empty":
                    continue
                elif parsed["command"] == "error":
                    print(f"[✗] {parsed['error']}")
                    continue
                elif parsed["command"] == "exit":
                    print("Exiting OpenTestability...")
                    break
                
                # Set verbose flag
                if hasattr(parsed["args"], 'verbose'):
                    self.verbose = parsed["args"].verbose
                
                # Execute command
                command = parsed["command"]
                args = parsed["args"]
                
                if command == "parse":
                    self.execute_parse(args)
                elif command == "scoap":
                    self.execute_scoap(args)
                elif command == "reconv":
                    self.execute_reconv(args)
                elif command == "simple":
                    self.execute_simple(args)
                elif command == "advanced":
                    self.execute_advanced(args)
                elif command == "compare":
                    self.execute_compare(args)
                elif command == "visualize":
                    self.execute_visualize(args)
                elif command == "help":
                    self.show_help(args.topic if hasattr(args, 'topic') else None)
                elif command == "status":
                    self.show_status()
                else:
                    print(f"[✗] Unknown command: {command}")
                    print("Type 'help' for available commands")
                
            except KeyboardInterrupt:
                print("\nType 'exit' to quit")
            except EOFError:
                print("\nExiting...")
                break
            except Exception as e:
                print(f"[✗] Unexpected error: {e}")


def main():
    """Main entry point."""
    if len(sys.argv) > 1:
        # Direct command execution
        env = OpenTestEnvironment()
        cmd_line = " ".join(sys.argv[1:])
        parsed = env.parse_command(cmd_line)
        
        if parsed["command"] == "error":
            print(f"[✗] {parsed['error']}")
            sys.exit(1)
        
        # Execute single command
        command = parsed["command"]
        args = parsed["args"]
        
        if command == "parse":
            success = env.execute_parse(args)
        elif command == "scoap":
            success = env.execute_scoap(args)
        elif command == "reconv":
            success = env.execute_reconv(args)
        elif command == "simple":
            success = env.execute_simple(args)
        elif command == "advanced":
            success = env.execute_advanced(args)
        elif command == "compare":
            success = env.execute_compare(args)
        elif command == "visualize":
            success = env.execute_visualize(args)
        elif command == "help":
            env.show_help(args.topic if hasattr(args, 'topic') else None)
            success = True
        elif command == "status":
            env.show_status()
            success = True
        else:
            print(f"[✗] Unknown command: {command}")
            sys.exit(1)
        
        sys.exit(0 if success else 1)
    else:
        # Interactive mode
        env = OpenTestEnvironment()
        env.run()


if __name__ == "__main__":
    main() 